{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FixedSense.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import six.moves.urllib as urllib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import speech_recognition as sr\n",
    "import cv2\n",
    "import pyttsx3\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the model and dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "MODEL_FILE = MODEL + '.tar.gz'\n",
    "DOWNLOAD_FOLDER = 'http://download.tensorflow.org/models/object_detection/'\n",
    "GRAPH_FILE = MODEL + '/frozen_inference_graph.pb'\n",
    "\n",
    "LABEL_FILE = 'mscoco_label_map.pbtxt'\n",
    "LABEL_PATH = os.path.join(os.getcwd(),'data',LABEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists :D\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(GRAPH_FILE):\n",
    "    print('Downloading the model')\n",
    "    opener = urllib.request.URLopener()\n",
    "    opener.retrieve(DOWNLOAD_FOLDER + MODEL_FILE, MODEL_FILE)\n",
    "    tar_file = tarfile.open(MODEL_FILE)\n",
    "    for file in tar_file.getmembers():\n",
    "        if 'frozen_inference_graph.pb' in os.path.basename(file.name):\n",
    "\t        tar_file.extract(file, os.getcwd())\n",
    "else:\n",
    "\tprint ('Model already exists :D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(LABEL_PATH)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, use_display_name=True, max_num_classes = 90)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(GRAPH_FILE, 'rb') as f:\n",
    "        serialized_graph = f.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dict = {}\n",
    "object_names = []\n",
    "vehicles = ['car', 'bus', 'truck']\n",
    "for i in range(len(categories)):\n",
    "    object_names.append(categories[i]['name'])\n",
    "    name = categories[i]['name']\n",
    "    if name in vehicles:\n",
    "        categories[i]['priority'] = 1\n",
    "    elif name == 'person':\n",
    "        categories[i]['priority'] = 2\n",
    "    else:\n",
    "        categories[i]['priority'] = 3\n",
    "    new_object = {'name':categories[i]['name'],'id':categories[i]['id'],'priority':categories[i]['priority']}\n",
    "    object_dict[categories[i]['id']] = new_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to speak messages using a seperate thread\n",
    "def create_thread(message, INIT_NUM_THREAD, engine, wait = 0.1):\n",
    "    if len(threading.enumerate()) <= INIT_NUM_THREAD:\n",
    "        say = threading.Thread(target = engine.say, args = (message,))\n",
    "        run = threading.Thread(target = engine.runAndWait)\n",
    "        wait = threading.Thread(target = time.sleep(wait))\n",
    "        say.start()\n",
    "        run.start()\n",
    "        wait.start()\n",
    "\n",
    "# utility function to perform main operation\n",
    "def calc_dist(image_np, object, boxes, scores, detected, mode, confident_cutoff = 0.6):\n",
    "    if scores[0][i] >= confident_cutoff:\n",
    "        # append 'detected' list\n",
    "        if mode == 'aware' or mode == 'search':\n",
    "            detected.append(object)\n",
    "        \n",
    "        # calculate distance\n",
    "        mid_x = (boxes[0][i][1] + boxes[0][i][3]) / 2\n",
    "        mid_y = (boxes[0][i][0] + boxes[0][i][2]) / 2\n",
    "        apx_distance = round(((1 - (boxes[0][i][3] - boxes[0][i][1])) ** 4), 1)\n",
    "        \n",
    "        # display text (mode) on screen\n",
    "        cv2.putText(image_np, '{}'.format(apx_distance),\n",
    "                    (int(mid_x * 800), int(mid_y * 450)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "        \n",
    "        # display warning if object is in a very close distance\n",
    "        if apx_distance <= 0.5:\n",
    "            if mid_x > 0.3 and mid_x < 0.7:\n",
    "                if mode == 'warn':\n",
    "                    detected.append(object)\n",
    "                cv2.putText(image_np, 'WARNING!!!', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 3)\n",
    "    return image_np\n",
    "\n",
    "# utility function to gently close the program\n",
    "def quit(cap):\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Recognition Handler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility class to handle speech recognition\n",
    "class SpeechRecognizer():\n",
    "    default_msg = \"start speaking and stop once you are done...\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone()\n",
    "        self.response = {\n",
    "            \"error\": None,\n",
    "            \"transcription\": \"\",\n",
    "        }\n",
    "        \n",
    "    def hear(self) -> dict[str, str]:\n",
    "        with self.microphone as source:\n",
    "            self.recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = self.recognizer.listen(source)\n",
    "        try:\n",
    "            self.response[\"transcription\"] = self.recognizer.recognize_google(audio)\n",
    "        except sr.RequestError:\n",
    "            self.response[\"error\"] = \"Cannot reach API\"\n",
    "        except sr.UnknownValueError:\n",
    "            self.response[\"error\"] = \"Speech unrecognizable\"\n",
    "        return self.response\n",
    "    \n",
    "    def get_input(self, message: str = default_msg) -> str:\n",
    "        print(message)\n",
    "        response = self.hear()\n",
    "        return response[\"transcription\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available Modes <br>\n",
    "1: Aware Mode <br>\n",
    "2: Warn Mode <br>\n",
    "3: Search Mode <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "[]\n",
      "[]\n",
      "['laptop']\n",
      "['book', 'book']\n",
      "['keyboard']\n",
      "['tv']\n",
      "[]\n",
      "['person']\n",
      "['tie']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['laptop']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['tie']\n",
      "[]\n",
      "[]\n",
      "['chair', 'bottle', 'chair', 'bottle']\n",
      "['person', 'chair', 'bottle', 'chair', 'bottle']\n",
      "['person', 'bottle', 'bottle', 'chair']\n",
      "['person', 'chair']\n",
      "['bottle']\n",
      "[]\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person']\n",
      "['person']\n",
      "['person', 'person']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'chair']\n",
      "['person']\n",
      "['person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person']\n",
      "['person']\n",
      "['person', 'person']\n",
      "['person']\n",
      "['person', 'person']\n",
      "['person', 'person', 'couch']\n",
      "['person', 'person', 'couch']\n",
      "['person', 'person', 'couch']\n",
      "['person', 'person']\n",
      "['person', 'couch']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person']\n",
      "['person', 'person']\n",
      "['person', 'chair']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'chair']\n",
      "['person', 'person', 'chair', 'chair']\n",
      "['person', 'person']\n",
      "['chair']\n",
      "['person', 'person']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person', 'chair']\n",
      "['person', 'person']\n",
      "['person', 'person', 'chair', 'bottle', 'chair']\n",
      "['person', 'person', 'chair', 'chair', 'bottle']\n",
      "['person', 'chair', 'chair', 'bottle']\n",
      "['person', 'chair', 'bottle', 'chair']\n",
      "['person', 'person', 'person', 'bottle', 'dining table']\n",
      "['person', 'bottle']\n",
      "['person', 'bottle']\n",
      "['person', 'bottle']\n",
      "['person', 'bottle']\n",
      "['person', 'bottle', 'dining table']\n",
      "['person', 'bottle']\n",
      "['person', 'bottle']\n",
      "['person', 'bottle']\n",
      "['bottle']\n",
      "['laptop', 'bottle']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['bottle']\n",
      "['bottle', 'laptop']\n",
      "['bottle']\n",
      "['bottle', 'laptop']\n",
      "['laptop', 'bottle']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "[]\n",
      "[]\n",
      "['keyboard']\n",
      "[]\n",
      "[]\n",
      "['keyboard']\n",
      "[]\n",
      "['keyboard']\n",
      "['laptop', 'keyboard']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['keyboard']\n",
      "[]\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['keyboard', 'laptop']\n",
      "['keyboard', 'laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['keyboard']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['keyboard']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['keyboard']\n",
      "['keyboard']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "['laptop']\n",
      "[]\n",
      "['laptop']\n",
      "[]\n",
      "['laptop']\n",
      "['laptop']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['laptop']\n",
      "['book']\n",
      "['laptop']\n",
      "['book']\n",
      "['laptop']\n",
      "['bottle', 'laptop']\n",
      "['bottle', 'laptop']\n",
      "['bottle', 'laptop', 'dining table']\n",
      "['bottle', 'laptop']\n",
      "['laptop', 'dining table']\n",
      "['bottle']\n",
      "['person', 'chair']\n",
      "['person', 'chair', 'bottle']\n",
      "['person', 'bottle']\n",
      "['person', 'bottle']\n",
      "['person']\n",
      "['person', 'person', 'bottle']\n",
      "['person', 'person', 'chair', 'chair', 'bottle']\n",
      "['person', 'person', 'chair', 'bottle']\n",
      "['chair', 'bottle']\n",
      "['person', 'person', 'chair', 'bottle']\n",
      "['person']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['dining table']\n",
      "['dining table']\n",
      "['dining table']\n",
      "['dining table']\n",
      "[]\n",
      "[]\n",
      "['dining table']\n",
      "['dining table']\n",
      "[]\n",
      "[]\n",
      "['dining table']\n",
      "['dining table']\n",
      "['dining table']\n",
      "['dining table']\n",
      "['dining table']\n",
      "['dining table']\n",
      "[]\n",
      "['dining table', 'handbag']\n",
      "['handbag']\n",
      "[]\n",
      "['dining table']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['dining table']\n",
      "['dining table']\n",
      "['dining table']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['dining table']\n",
      "['dining table']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['cat']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['bottle']\n",
      "['person', 'bottle']\n",
      "[]\n",
      "['bottle']\n",
      "[]\n",
      "['person', 'book']\n",
      "['person', 'laptop']\n",
      "['laptop']\n",
      "['laptop', 'bottle']\n",
      "['bottle', 'laptop']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['laptop']\n",
      "['laptop']\n"
     ]
    }
   ],
   "source": [
    "url = \"http://10.27.234.91:8080/video\" # change this depending on your url\n",
    "cap = cv2.VideoCapture(url)\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "AVAILABLE_MODES = ['aware', 'warn', 'search']\n",
    "INIT_NUM_THREAD = len(threading.enumerate())\n",
    "USE_SPEECH = False # toggle this to use speech recognition\n",
    "\n",
    "# utility function to choose mode based on speech recognition\n",
    "def choose_mode():\n",
    "    chosen_mode = None\n",
    "    if USE_SPEECH:\n",
    "        while chosen_mode == None:\n",
    "            mode_input = SpeechRecognizer().get_input()\n",
    "            print(mode_input)\n",
    "            chosen_mode = [m for m in AVAILABLE_MODES if m in mode_input]\n",
    "            chosen_mode = chosen_mode[0] if chosen_mode else None\n",
    "    else:\n",
    "        chosen_mode = input(\"Enter mode: \")\n",
    "        mode_input = \"\"\n",
    "        while chosen_mode.strip().lower() not in AVAILABLE_MODES:\n",
    "            chosen_mode = input(\"Enter mode: \")\n",
    "    return chosen_mode, mode_input\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.compat.v1.Session(graph=graph) as sess:\n",
    "        create_thread('Starting camera, choose your mode', INIT_NUM_THREAD, engine) \n",
    "        \n",
    "        # initialize looping variables\n",
    "        iterator, temp = 80, 0\n",
    "        input_search = True\n",
    "        search_item = None\n",
    "        detected = []\n",
    "        mode = None\n",
    "        \n",
    "        # choose mode\n",
    "        create_thread('Choose your mode', INIT_NUM_THREAD, engine)\n",
    "        mode, transcripts = choose_mode()\n",
    "        create_thread(f'Entering {mode} mode...', INIT_NUM_THREAD, engine)\n",
    "\n",
    "        while True:\n",
    "            # stopping the program when 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                quit(cap)\n",
    "                break\n",
    "            \n",
    "            # choose mode when 'c' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "                print(\"Switch mode\")\n",
    "                create_thread(f'Switch mode...', INIT_NUM_THREAD, engine)\n",
    "                mode, transcripts = choose_mode()\n",
    "\n",
    "            # initialize the graph\n",
    "            return_value, image_np = cap.read()\n",
    "            ret,image_np = cap.read()\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            image_tensor = graph.get_tensor_by_name('image_tensor:0')\n",
    "            boxes = graph.get_tensor_by_name('detection_boxes:0')\n",
    "            scores = graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = graph.get_tensor_by_name('num_detections:0')\n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "            # visualizing the detection\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index ,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=8)\n",
    "\n",
    "            # list objects detected\n",
    "            detected.clear()\n",
    "            for i, b in enumerate(boxes[0]):\n",
    "                image_np = calc_dist(image_np, object_dict[classes[0][i]], boxes, scores, detected, mode = mode)\n",
    "                detected_object_list = [d['name'] for d in sorted(detected, key = lambda x: x['priority'])]\n",
    "\n",
    "            # add mode title\n",
    "            cv2.putText(image_np, f'{mode.capitalize()} Mode', (400,50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,0), 3)\n",
    "\n",
    "            # mode selection\n",
    "            if mode == 'aware':\n",
    "                NO_TOP_PICKS = 3\n",
    "                print(detected_object_list)\n",
    "                create_thread(' '.join(detected_object_list[:NO_TOP_PICKS]), INIT_NUM_THREAD, engine) \n",
    "\n",
    "            elif mode == 'warn':\n",
    "                #say = threading.Thread(target = engine.say, args = (message,))\n",
    "                if len(detected_object_list) == 1:\n",
    "                    create_thread(f'Warning, {detected_object_list[0]} very close', INIT_NUM_THREAD, engine)\n",
    "\n",
    "            elif mode == 'search':\n",
    "                # determine search item\n",
    "                if input_search == True:\n",
    "                    # search based on the object that the user has said\n",
    "                    object_names = [val[\"name\"] for _, val in object_dict.items()]\n",
    "                    for name in object_names:\n",
    "                        if name in transcripts:\n",
    "                            search_item = name\n",
    "                    \n",
    "                    # prompt the user for what item to search (if user does not mention it yet)\n",
    "                    if not search_item:\n",
    "                        search_item = input(\"Enter search item: \")\n",
    "                        while search_item not in object_names:\n",
    "                            search_item = input(\"Enter search item: \")\n",
    "                        create_thread(f\"Searching for {search_item}\", INIT_NUM_THREAD, engine)\n",
    "                input_search = False\n",
    "                \n",
    "                # check if search_item is in the detected_object_list, exit program when found\n",
    "                found = search_item in detected_object_list\n",
    "                if found: temp += 1\n",
    "                elif temp >= 1:\n",
    "                    create_thread(f\"{search_item} found! Exiting search mode.\", INIT_NUM_THREAD, engine)\n",
    "                    temp += 1\n",
    "                if temp == 50:\n",
    "                    quit(cap)\n",
    "                    break\n",
    "                \n",
    "                # periodically say the item currently being searched every LOOP_FRAMES times\n",
    "                LOOP_FRAMES = 100\n",
    "                iterator += 1\n",
    "                if iterator == LOOP_FRAMES:\n",
    "                    create_thread(f\"Still searching for {search_item}\", INIT_NUM_THREAD, engine)\n",
    "                    iterator = 0\n",
    "\n",
    "            # Show the image\n",
    "            cv2.imshow('image', cv2.resize(image_np, (1024, 768)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e821daa3f59c678465e8a130251f24feb2fff2592c3bc8f1f1befc95baefb969"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
