{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import six.moves.urllib as urllib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import pyttsx3\n",
    "from utils import visualization_utils as vis_util\n",
    "import threading\n",
    "import time\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the model and dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "MODEL_FILE = MODEL + '.tar.gz'\n",
    "DOWNLOAD_FOLDER = 'http://download.tensorflow.org/models/object_detection/'\n",
    "GRAPH_FILE = MODEL + '/frozen_inference_graph.pb'\n",
    "\n",
    "LABEL_FILE = 'mscoco_label_map.pbtxt'\n",
    "LABEL_PATH = os.path.join(os.getcwd(),'data',LABEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists :D\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(GRAPH_FILE):\n",
    "    print('Downloading the model')\n",
    "    opener = urllib.request.URLopener()\n",
    "    opener.retrieve(DOWNLOAD_FOLDER + MODEL_FILE, MODEL_FILE)\n",
    "    tar_file = tarfile.open(MODEL_FILE)\n",
    "    for file in tar_file.getmembers():\n",
    "        if 'frozen_inference_graph.pb' in os.path.basename(file.name):\n",
    "\t        tar_file.extract(file, os.getcwd())\n",
    "else:\n",
    "\tprint ('Model already exists :D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_map(label_map_path, default_priority = 3, default_width = 0.1):\n",
    "    items = {}\n",
    "    with open(label_map_path, \"r\") as file:\n",
    "        \n",
    "        item_id = None\n",
    "        item_name = None\n",
    "        item_priority = default_priority\n",
    "        item_width = default_width\n",
    "\n",
    "        for line in file:\n",
    "            line.replace(\" \", \"\")\n",
    "            if \"item{\" in line:\n",
    "                pass\n",
    "            elif  \"}\" in line:\n",
    "                items[item_id] = {'id':item_id,\n",
    "                                  'name':item_name,\n",
    "                                  'priority':item_priority,\n",
    "                                  'width':item_width}\n",
    "                item_id = None\n",
    "                item_name = None\n",
    "                item_priority = default_priority\n",
    "                item_width = default_width\n",
    "            elif \"id:\" in line:\n",
    "                item_id = int(line.split(\":\", 1)[1].strip())\n",
    "            elif \"name:\" in line:\n",
    "                item_name = line.split(\":\", 1)[1].replace('\"', \"\").strip()\n",
    "            elif \"priority:\" in line:\n",
    "                item_priority = int(line.split(\":\", 1)[1].strip())\n",
    "            elif \"width:\" in line:\n",
    "                item_width = float(line.split(\":\", 1)[1].strip())\n",
    "    return items\n",
    "\n",
    "object_dict = read_label_map(LABEL_PATH)\n",
    "object_names = []\n",
    "for e in object_dict:\n",
    "    object_names.append(object_dict[e]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(GRAPH_FILE, 'rb') as f:\n",
    "        serialized_graph = f.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Recognition Handler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility class to handle speech recognition\n",
    "class SpeechRecognizer():\n",
    "    default_msg = \"start speaking and stop once you are done...\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone()\n",
    "        self.response = {\n",
    "            \"error\": None,\n",
    "            \"transcription\": \"\",\n",
    "        }\n",
    "        \n",
    "    def hear(self) -> dict[str, str]:\n",
    "        with self.microphone as source:\n",
    "            self.recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = self.recognizer.listen(source)\n",
    "        try:\n",
    "            self.response[\"transcription\"] = self.recognizer.recognize_google(audio)\n",
    "        except sr.RequestError:\n",
    "            self.response[\"error\"] = \"Cannot reach API\"\n",
    "        except sr.UnknownValueError:\n",
    "            self.response[\"error\"] = \"Speech unrecognizable\"\n",
    "        return self.response\n",
    "    \n",
    "    def get_input(self, message: str = default_msg) -> str:\n",
    "        print(message)\n",
    "        response = self.hear()\n",
    "        return response[\"transcription\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new thread for the engine voice message\n",
    "def create_thread(message, INIT_NUM_THREAD, engine, wait = 0.1):\n",
    "    if len(threading.enumerate()) <= INIT_NUM_THREAD:\n",
    "        say = threading.Thread(target = engine.say, args = (message,))\n",
    "        run = threading.Thread(target = engine.runAndWait)\n",
    "        end = threading.Thread(target = engine.endLoop)\n",
    "        wait = threading.Thread(target = time.sleep(wait))\n",
    "        say.start()\n",
    "        run.start()\n",
    "        end.start()\n",
    "        wait.start()\n",
    "\n",
    "# depth perception calculations\n",
    "def calc_dist(image_np, index, object, boxes, scores, detected, mode, confident_cutoff = 0.6, dist_0 = 1):\n",
    "    if scores[0][index] >= confident_cutoff:\n",
    "        mid_x = (boxes[0][index][1]+boxes[0][index][3])/2\n",
    "        mid_y = (boxes[0][index][0]+boxes[0][index][2])/2\n",
    "        perceived_width = abs(boxes[0][index][3] - boxes[0][index][1])\n",
    "        scale = object['width'] / perceived_width\n",
    "        apx_distance = dist_0 * scale\n",
    "        apx_distance = round(apx_distance,1)\n",
    "        object['distance'] = apx_distance\n",
    "\n",
    "        if mode == 'aware' or mode == 'search':\n",
    "            detected.append(object)\n",
    "\n",
    "        cv2.putText(image_np, '{} m'.format(apx_distance), (int(mid_x*800),int(mid_y*450)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "        \n",
    "        if apx_distance <= 1:\n",
    "            if mode == 'warn':                    \n",
    "                detected.append(object)\n",
    "                cv2.putText(image_np, 'WARNING!!!', (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 3)\n",
    "    return image_np\n",
    "    \n",
    "def quit(cap):\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available Modes <br>\n",
    "1: Aware Mode <br>\n",
    "2: Warn Mode <br>\n",
    "3: Search Mode <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = '0.0.0.0' # for mobile connection, change ip here\n",
    "url = 'http://' + IP + ':8080/video'\n",
    "cap = cv2.VideoCapture(0)\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "AVAILABLE_MODES = ['aware','warn','search']\n",
    "INIT_NUM_THREAD = len(threading.enumerate())\n",
    "USE_SPEECH = False # toggle this to use speech recognition\n",
    "SEARCH_LOOP_FRAMES = 100\n",
    "QUIT_DELAY = 50\n",
    "\n",
    "# utility function to choose mode based on speech recognition\n",
    "def choose_mode():\n",
    "    chosen_mode = None\n",
    "    if USE_SPEECH:\n",
    "        while chosen_mode == None:\n",
    "            mode_input = SpeechRecognizer().get_input()\n",
    "            print(mode_input)\n",
    "            chosen_mode = [m for m in AVAILABLE_MODES if m in mode_input]\n",
    "            chosen_mode = chosen_mode[0] if chosen_mode else None\n",
    "    else:\n",
    "        chosen_mode = input(\"Enter mode: \")\n",
    "        mode_input = \"\"\n",
    "        while chosen_mode.strip().lower() not in AVAILABLE_MODES:\n",
    "            chosen_mode = input(\"Enter mode: \")\n",
    "    return chosen_mode, mode_input\n",
    "\n",
    "mode, transcripts = choose_mode()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.compat.v1.Session(graph=graph) as sess:\n",
    "        \n",
    "        # Initialize looping variables\n",
    "        iterator = 0\n",
    "        input_search = True\n",
    "        search_item = None\n",
    "        detected = []\n",
    "        quit_counter = 0\n",
    "\n",
    "        while True:\n",
    "            # stopping the program when 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                quit(cap)\n",
    "                break\n",
    "\n",
    "            # initialize the graph\n",
    "            return_value, image_np = cap.read()\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            image_tensor = graph.get_tensor_by_name('image_tensor:0')\n",
    "            boxes = graph.get_tensor_by_name('detection_boxes:0')\n",
    "            scores = graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = graph.get_tensor_by_name('num_detections:0')\n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict = {image_tensor: image_np_expanded})\n",
    "\n",
    "            # visualizing the detection\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                object_dict,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=5)\n",
    "\n",
    "            # list objects detected\n",
    "            detected.clear()\n",
    "            for i, b in enumerate(boxes[0]):\n",
    "                image_np = calc_dist(image_np, i, object_dict[classes[0][i]], boxes, scores, detected, mode = mode)\n",
    "                object_distance = {d['name']:d['distance'] for d in sorted(detected, key = lambda x: x['priority'])}\n",
    "            \n",
    "            object_found = []\n",
    "            for e in object_distance:\n",
    "                object_found.append(e)\n",
    "\n",
    "            # add mode title\n",
    "            cv2.putText(image_np, f'{mode.capitalize()} Mode', (400,50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,0), 3)\n",
    "\n",
    "            # mode selection\n",
    "            if mode == 'aware':\n",
    "                NO_TOP_PICKS = 3\n",
    "                create_thread(' '.join(object_found[:NO_TOP_PICKS]), INIT_NUM_THREAD, engine) \n",
    "\n",
    "            elif mode == 'warn':\n",
    "                if len(object_found) != 0:\n",
    "                    create_thread(f'Warning, {object_found[0]} very close', INIT_NUM_THREAD, engine)\n",
    "\n",
    "            elif mode == 'search':\n",
    "                # determine search item\n",
    "                if input_search == True:\n",
    "                    if USE_SPEECH:\n",
    "                        # search based on the object that the user has said\n",
    "                        for name in object_names:\n",
    "                            if name in transcripts:\n",
    "                                search_item = name\n",
    "                    \n",
    "                    # prompt the user for what item to search (if user does not mention it yet)\n",
    "                    if not search_item:\n",
    "                        search_item = input(\"Enter search item: \")\n",
    "                        while search_item not in object_names:\n",
    "                            search_item = input(\"Enter search item: \")\n",
    "                    \n",
    "                    create_thread(f\"Searching for {search_item}\", INIT_NUM_THREAD, engine)\n",
    "                input_search = False\n",
    "                \n",
    "                # check if search_item is in the object_found, exit program when found\n",
    "                # quit counter to delay the quitting\n",
    "                found = search_item in object_found\n",
    "                if found and quit_counter==0: \n",
    "                    create_thread(f\"{search_item} found {object_distance[search_item]} meters away! Exiting search mode.\", INIT_NUM_THREAD, engine)\n",
    "                    quit_counter = 1\n",
    "                elif quit_counter >= 1:\n",
    "                    quit_counter += 1\n",
    "                if quit_counter == 50:\n",
    "                    quit(cap)\n",
    "                    break\n",
    "\n",
    "                # periodically say the item currently being searched every LOOP_FRAMES times\n",
    "                iterator += 1\n",
    "                if iterator == SEARCH_LOOP_FRAMES:\n",
    "                    create_thread(f\"Still searching for {search_item}\", INIT_NUM_THREAD, engine)\n",
    "                    iterator = 0\n",
    "\n",
    "            # Show the image\n",
    "            cv2.imshow('image',cv2.resize(image_np,(1024,768)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e821daa3f59c678465e8a130251f24feb2fff2592c3bc8f1f1befc95baefb969"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
